# 训练稳定性与“训进去”的判断

本文档说明如何判断模型是否在稳步变强、为何会出现「对 Random 胜率下降、和棋率上升」，以及可采取的稳健策略（类似 KataGo 的“多多益善”）。

## 1. 为何对 Random 胜率会下降、和棋率上升？

常见现象：前几轮对 Random 胜率约 10%，到第 3～4 轮降到约 3%，几乎不输，和棋比例明显上升。

可能原因（可同时存在）：

- **自博弈数据里和棋变多**  
  无吃子 36 步判和 + 模型变强 → 对局更容易拖到判和。训练目标里「和棋」样本变多，value 会学成“很多局面是 0”，策略上更倾向安全、不冒险，对 Random 时更容易走成和棋而不是争胜。
- **“不输”优先于“赢”**  
  对 Random 时 MCTS 选的是“期望结果最好”的着法。若 value 对和棋（0）估计很准、对赢（+1）方差大，策略会偏向不输/和棋。
- **draw 权重**  
  `value_draw_weight=0.1`、`policy_draw_weight=0.3` 只是压低和棋样本的梯度，并没有删掉；和棋样本数量大时，仍会拉低“争胜”信号。

因此：**单看「对 Random 胜率」会误导**——胜率下降不一定代表变弱，可能是模型更保守、更少输、更多和。要判断是否“训进去”，需要更多指标。

## 2. 如何判断“能训进去”？

建议同时看以下几类信号（由主到辅）：

1. **对上一迭代的胜率（推荐）**  
   - 使用 `--eval_games_vs_previous N`（例如 100 或 200），每轮让当前模型与**上一轮 checkpoint** 对局。  
   - **若 win_rate_vs_previous > 50%**：说明当前模型比上一轮强，训练在正向推进。  
   - 即使对 Random 胜率下降，只要对上一迭代稳定 >50%，就可以认为在稳步变强（KataGo 等也主要看对过去版本的胜率/ELO）。

2. **自博弈结果分布**  
   - 训练日志里已打印：`Self-play outcomes: W wins, L losses, D draws (draw rate …%)`。  
   - 若和棋率随迭代快速升高（例如 >80%），说明数据里和棋占主导，value 容易学成“多数是和”，不利于学“争胜”。可配合下面“稳健策略”做数据或目标上的调整。

3. **对 best_model 的胜率**  
   - 若当前模型能超过 `win_rate_threshold` 击败 best，则更新 best；这是“绝对强度”的参考，但波动较大，适合与“对上一迭代”一起看。

4. **Loss 曲线**  
   - policy / value loss 平稳下降、无长期震荡或爆炸，说明优化在收敛；若 loss 已很低但对上一迭代胜率不升，再考虑数据或奖励设计。

5. **人机对弈 / 下载权重对局**  
   - 作为最终体感检验；不适合做自动化稳定性判断，可与上述指标一起用。

## 3. 稳步推进的推荐做法（“多多益善”式）

- **不要只用 decisive_only**  
  只用赢/输会丢掉大量和棋样本，数据分布偏、训练容易不稳。建议保留全部数据，用 `value_draw_weight` / `policy_draw_weight` 压低和棋权重即可（例如当前 0.1 / 0.3）。

- **打开“对上一迭代”评估**  
  在脚本中加上例如：  
  `--eval_games_vs_previous 100`  
  每轮看 `Challenger win rate vs previous iter`；若多数轮次 >50%，说明在稳步变强。

- **关注自博弈和棋率**  
  若某几轮后 `Self-play outcomes` 里 draw rate 很高（如 >70%），可考虑其一或组合：  
  - 略提前 resign（如 `--self_play_resign_threshold -0.9`），让更多对局在“已明显劣势”时结束，减少冗长和棋；  
  - 或适当提高 `value_draw_weight`（例如 0.2）让和棋的 value 目标略弱一点（仍保留样本）；  
  - 或后续在 value 目标上对“赢”做轻微正向偏置（需改代码，这里仅作方向）。

- **学习率与 batch**  
  若对上一迭代胜率波动大或长期不升，可尝试略降学习率（如 `--lr 0.001`）或适当增大 batch，观察几轮再调。

- **best 的更新条件（当前实现）**  
  - 对 Random 胜率 **≥ 55%**（`--win_rate_threshold 0.55`，胜率 = wins/all）才进入“和 best 比较”的流程。  
  - 晋升为 best 需同时满足：对 Random ≥55%；对 **previous** 胜率 > 败率（即 wins/all > losses/all，有大量和局时只要赢的局数多于输的即可）；对 **best** 同样要求胜率 > 败率。  
  - 这样既保证 best 一定强于 Random，又避免“对 best 必须 55%”过难；和局多时只要赢多输少即可晋升。

## 4. 小结

- **“训进去”的主要依据**：**对上一迭代胜率 >50%**，其次才是对 Random / 对 best 的胜率和 loss。  
- **对 Random 胜率下降、和棋率上升**：多半是数据与价值学习更偏“不输/和棋”，不一定代表变弱；配合“对上一迭代”和自博弈结果分布一起看更准确。  
- **稳健策略**：用全部数据 + draw 权重、打开 `--eval_games_vs_previous`、监控自博弈和棋率与 loss，再按需微调 resign/学习率，即可向“多多益善、稳步变强”靠拢。
