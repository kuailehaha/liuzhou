# 基于 MCTS 的强化学习六洲棋 AI 系统实现

本项目聚焦传统棋类“六洲棋”的智能对弈系统，涵盖规则建模、MCTS 强化学习训练与人机对战系统实现。

[六洲棋][1]是广泛分布于中国北方的一种传统棋类。该棋类规则各地不一，仅以山东中部地方规则作为实现标准。具体规则叙述见[规则文档]。(./rule_description.md) 

[1]: https://zh.wikipedia.org/wiki/%E6%96%B9%E6%A3%8B

任何建议或帮助，请联系[我](mailto:kuailepapa@gmail.com)。


## 📰 里程碑（News）

* 2025-10-29：完成 legacy 主体框架搭建并通过运行验证。

  * GeForce RTX 3060训练3小时，即可以81%-11%-8% (win-draw-lose，200 eval game，最大允许回合500) 战胜 Random Agent。

* 2025-12-27：完成 v0 主体框架搭建并通过运行验证。

  * 使用C++/CUDA重写，在训练效果不降低的前提下，将自博弈速度提升至原来的~16倍，速度瓶颈从规则引擎/合法动作生成转移到神经网络前向推理。

* 2026-02-26：v1 训练主线进入“强度迭代”阶段（大规模训练 + 锦标赛验证）。

  * 测试 `vs_random`胜率达到 `99.80%`（1000 局，最大允许回合144，超过36回合双方没有子力减损判为平局）。
  * 同一训练中，自博弈 `decisive_games` 从 `1.23%` 提升到 `81.72%`，有效胜负样本显著增加。
  * 80 模型锦标赛冠军为 `model_iter_032.pt`（`logs/v1_tournament_80models.json`）。
  * v1 相对 v0 的速度提升在验证集中达到 `~25x-28x`，训练吞吐已达到单节点集群可扩展规模。

## 🎯 当前目标（2026-02）

1. 训练出人类难以战胜的强 Agent。
2. 在训练链路引入 LR scheduler，改善“数据量增加但棋力不随之提升”的问题。
3. 继续扩展棋力评估体系：保留 `vs_random`，同时以锦标赛 + Elo/BT 排名作为判据，减少单指标偏差。

---

# 规则说明（Rules）

本文档记录了在重构后（将每个动作拆分为原子阶段）所采用的最新规则集。
以下内容取代了旧版本中将多个决策捆绑为单个动作的描述。

---

## 🎮 游戏概述（Game Overview）

* **棋盘**：6 × 6 网格（共 36 个交叉点）。
* **棋子**：两种颜色，`BLACK`（黑）与 `WHITE`（白），每方初始各有 18 枚棋子。
* **目标**：通过吃子移除对方所有棋子。
  如果在限定步数内双方都未能达成此目标，或走子阶段连续无吃子达到一定步数，游戏以平局结束。

---

## ⚙️ 阶段结构（原子动作）

游戏在任意时刻都处于下列阶段之一。
每个阶段会持续到所有待处理的子任务完成后，才会交由下一位玩家或进入下一阶段。

---

### 1️⃣ `PLACEMENT`（落子阶段）

* 当前玩家在一个**空的交叉点**上放置一枚棋子，
  但不能放在目前被对方标记的位置上。
* 放置完成后，检查新棋子（若其位置之前未被标记）是否形成以下结构：

  * **正方形**：2×2 方块 → 生成 **1 个** 对方棋子标记任务；
  * **直线**：同行或同列 6 枚棋子（忽略标记棋） → 生成 **2 个** 对方棋子标记任务。
* 若存在待处理标记 → 进入 `MARK_SELECTION` 阶段；
  否则：

  * 若棋盘已满 → 进入 `REMOVAL` 阶段；
  * 若未满 → 轮到对手，继续 `PLACEMENT`。

---

### 2️⃣ `MARK_SELECTION`（标记选择阶段）

* 逐个处理待处理的标记任务。
* 当前玩家为每个待标记任务选择一个合法的对方棋子：

  * 合法目标为**未被标记的对方棋子**；
  * 当存在未构成方块/直线的对方棋时，**优先选择这些棋子**。
* 当所有标记任务完成后：

  * 若棋盘已满 → 进入 `REMOVAL`；
  * 否则交换玩家并返回 `PLACEMENT`。

---

### 3️⃣ `REMOVAL`（移除阶段）

* 若有被标记的棋子 → **同时移除双方所有被标记的棋子**，
  然后进入 `MOVEMENT` 阶段，由 `WHITE` 先行。
* 若没有标记（棋盘填满但无触发标记），则进入 **强制移除序列**（`FORCED_REMOVAL`）：

  1. `WHITE` 先移除一枚不属于方块/直线结构的 `BLACK` 棋子；
  2. 然后 `BLACK` 移除一枚符合同样规则的 `WHITE` 棋子；
  3. 随后进入 `MOVEMENT` 阶段，由 `WHITE` 先行。

---

### 4️⃣ `MOVEMENT`（移动阶段）

* 当前玩家选择并移动自己的一枚棋子，沿**正交方向**（上下左右）移动一步至相邻空位。
* 每次移动后检查是否形成新结构：

  * 方块 → 待捕获数 +1；
  * 直线 → 待捕获数 +2。
* 若存在待捕获任务 → 进入 `CAPTURE_SELECTION`；
  否则交换玩家并继续 `MOVEMENT`。
* **无可移动规则（No-move rule）**：
  若玩家无合法移动，则可以移除一枚不在方块/直线中的对方棋（若无“普通”棋，则任意移除一枚）。
  该操作通过特殊动作 `no_moves_remove` 实现，随后进入 `CAPTURE_SELECTION` 阶段由对方执行**反移除**。

---

### 5️⃣ `CAPTURE_SELECTION`（吃子选择阶段）

* 逐个执行待捕获任务。
* 合法目标与标记阶段一致：优先移除未构成方块/直线的对方棋。
* 每次吃子立即从棋盘移除。
* 若对方棋子全部被移除 → 当前玩家立即获胜。
  否则当所有捕获完成后 → 交换玩家并返回 `MOVEMENT`。

---

### 6️⃣ `COUNTER_REMOVAL`（反移除阶段）

* 仅在执行 `no_moves_remove` 动作后触发。
* 由对手（之前无法行动的一方）移除刚才行动玩家的一枚棋子，遵循相同合法性规则。
* 反移除完成后，若该玩家仍有棋子，则返回 `MOVEMENT` 阶段并交回回合。

---

## 🏁 游戏结束条件（End-of-Game Conditions）

* **吃子胜利**：若某次吃子（包括反移除）使对方棋子全部消失，则当前玩家立即获胜。
* **步数上限平局**：总步数达到 **144 步**（`MAX_MOVE_COUNT = 144`）仍无人获胜，则判定平局（双方得分为 0）。
* **无吃子判和**：进入走子阶段（`MOVEMENT`）后，若连续 **36 步**（`NO_CAPTURE_DRAW_LIMIT = 36`，即 18 个完整回合）双方均未发生吃子，则判定平局。`GameState` 通过 `moves_since_capture` 计数器追踪此状态，每次吃子（棋子总数减少）时重置为 0，否则递增。
* **无合法动作**：若 `generate_all_legal_moves` 返回空列表，则该玩家判负（通常意味着无子可下或早期阶段无法落子）。

---

## 📘 附加说明（Additional Notes）

* 被标记的棋子会在阶段 1（落子阶段）中保留在棋盘上，直到后续的 `REMOVAL` 阶段才被移除。
  被标记棋不可用于形成新的方块或直线。
* 当落子阶段结束且没有任何标记时，必须先执行**强制移除**，再进入移动阶段。
* “无可移动”与“反移除”机制可防止僵局：若某方完全阻挡对手，则给予对手一次移除机会，随后执行反移除，再恢复正常回合。
* 实现层面上，每个 `GameState` 都维护**待标记数**、**待捕获数**和**无吃子步数计数器**（`moves_since_capture`），
  确保在 `MARK_SELECTION` 与 `CAPTURE_SELECTION` 阶段中，每次仅处理一个原子任务，并在走子阶段追踪无吃子判和条件。
* `moves_since_capture` 在 Python (`src/game_state.py`) 和 C++ (`v0/include/v0/game_state.hpp`) 中均有实现，并已集成到 `TensorStateBatch` 和 CUDA 内核（`fast_apply_moves`）的批量状态追踪中。

---

该规则集与当前代码逻辑（`src/rule_engine.py`、`src/move_generator.py`、`v0/src/rules/rule_engine.cpp`、`v0/src/moves/move_generator.cpp`、以及 Monte Carlo 搜索模块）一致。
在编写测试、调试游戏逻辑或将引擎移植到其他语言时，请以此文档为权威参考。


## 🚀 V1 训练与评估入口（当前主线）

- 训练主脚本：`scripts/big_train_v1.sh`
- 统一入口：`scripts/train_entry.py --pipeline v1`
- 单点评估：`scripts/eval_checkpoint.py --backend v1`
- 锦标赛评估：`scripts/tournament_v1_eval.py`
- 本地快速回归（Windows）：`scripts/local_train_v1_3iter.ps1`

建议优先使用 staged 流程（`selfplay -> train -> eval -> infer`）进行大规模训练与验证。

---

## 📂 代码结构（当前维护）

- `v1/`：当前训练主线（self-play、训练桥接、MCTS GPU、轨迹缓存、设计文档）。
- `scripts/`：训练调度、评估、锦标赛与导出脚本。
- `tools/`：性能剖析、验证与诊断工具。
- `tests/`：规则回归、集成测试与 v1 流水线 smoke 测试。
- `src/`、`v0/`：保留用于规则参考、功能验证与底层能力复用，不作为当前训练主叙事。
